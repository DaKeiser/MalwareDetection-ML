#!/usr/bin/env python
# coding: utf-8

# In[35]:
# Import necessary packages

import pandas as pd
import numpy as np
import seaborn as sns

from imblearn.over_sampling import SMOTE

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose  import ColumnTransformer, make_column_selector
from sklearn.pipeline import make_pipeline
from sklearn.metrics import roc_auc_score, plot_roc_curve
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold
from sklearn.impute import SimpleImputer
import category_encoders as ce
from xgboost import XGBClassifier
from imblearn.pipeline import Pipeline
import multiprocessing


import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt 

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))


# In[36]:

# Do necessary preprocessing

df = pd.read_csv("/kaggle/input/malware-detection-tejas/train.csv")
df.set_index('MachineIdentifier', inplace=True)
target = 'HasDetections'
# df.drop(target, axis=1, inplace=True)

test_df = pd.read_csv("/kaggle/input/malware-detection-tejas/test.csv")
test_ids = test_df['MachineIdentifier']
test_df.set_index('MachineIdentifier', inplace=True)

# Dropping columns with more than 60% null values or just 1 unique value

df.drop(['PuaMode', 'Census_ProcessorClass', 'DefaultBrowsersIdentifier', 'Census_IsFlightingInternal', 'Census_InternalBatteryType', 'Census_ThresholdOptIn', 'Census_IsWIMBootEnabled'], axis = 1, inplace = True)
test_df.drop(['PuaMode', 'Census_ProcessorClass', 'DefaultBrowsersIdentifier', 'Census_IsFlightingInternal', 'Census_InternalBatteryType', 'Census_ThresholdOptIn', 'Census_IsWIMBootEnabled'], axis = 1, inplace = True)


df['Census_PrimaryDiskTypeName'].replace(to_replace = ['UNKNOWN', 'Unspecified'], inplace = True)
test_df['Census_PrimaryDiskTypeName'].replace(to_replace = ['UNKNOWN', 'Unspecified'], inplace = True)

df['Census_PrimaryDiskTypeName'].replace(to_replace = ['HDD', 'SSD'], value = [0, 1], inplace = True)
test_df['Census_PrimaryDiskTypeName'].replace(to_replace = ['HDD', 'SSD'], value = [0, 1], inplace = True)

print("Data Shape", df.shape)
df.head()


# In[37]:

# Scikit-Learn imputers require that the missing values are represented with np.nan hence use of the fillna method.

df.fillna(np.nan, inplace=True)
test_df.fillna(np.nan, inplace=True)


# In[38]:

# Imputing numerical columns

select_numeric_features = make_column_selector(dtype_include=np.number)
numeric_features = select_numeric_features(df)

numeric_pipeline = make_pipeline(SimpleImputer(strategy='median', add_indicator=True))


# In[39]:

# Selecting columns in one hot encoding list which have <= 10 unique values

MAX_OH_CARDINALITY = 10

def select_oh_features(df):
    
    hc_features =        df        .select_dtypes(['object', 'category'])        .apply(lambda col: col.nunique())        .loc[lambda x: x <= MAX_OH_CARDINALITY]        .index        .tolist()
        
    return hc_features

oh_features = select_oh_features(df)

print(f'N oh_features: {len(oh_features)} \n')
print(', '.join(oh_features))


# In[40]:

# Selecting columns in target encoding list which have > 10 unique values

def select_hc_features(df):
    
    hc_features =        df        .select_dtypes(['object', 'category'])        .apply(lambda col: col.nunique())        .loc[lambda x: x > MAX_OH_CARDINALITY]        .index        .tolist()
        
    return hc_features


hc_features = select_hc_features(df)

print(f'N hc_features: {len(hc_features)} \n')
print(', '.join(hc_features))


# In[41]:

# One hot encoding pipeline


oh_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore'))


# In[42]:


# Cat Boost Encoding pipeline

hc_pipeline = make_pipeline(ce.CatBoostEncoder())


# In[43]:

# Column Transformation pipeline


column_transformer = ColumnTransformer(transformers=                                       [('numeric_pipeline', numeric_pipeline, select_numeric_features),                                        ('oh_pipeline', oh_pipeline, select_oh_features),                                        ('hc_pipeline', hc_pipeline, select_hc_features)],
                                       n_jobs = multiprocessing.cpu_count(),
                                       remainder='drop')


# In[44]:

# Removing target column from the list of training columns

train_columns = list(df.columns)
train_columns.remove(target)
print(len(train_columns))


# In[ ]:


params = {}

# Uncomment this line below with the model of your choice. If it has class_weights parameter, 
# remove the SMOTE from pipeline and add class_weights = 'balanced' in it

'''
model = some_model(**params)
'''

model_pipeline = Pipeline([
        ('transform', column_transformer), 
        ('sampling', SMOTE(random_state=42)),
        ('somemodel', model)
    ])

# Fit the model

model_pipeline.fit(df[train_columns], df[target])

# Predict

y_train_pred = model_pipeline.predict_proba(test_df)


# In[ ]:


y_train_pred


# In[ ]:

# Find the predictions for 1's

preds = y_train_pred.transpose()[1]


# In[ ]:


res = pd.DataFrame()
res['MachineIdentifier'] = test_ids
res['HasDetections'] = preds
res


# In[ ]:

# Check for counts

res[res['HasDetections'] > 0.5].value_counts().sum()


# In[ ]:

# Submission CSV


res.to_csv('test_model_probas_XGB.csv',index=False)

