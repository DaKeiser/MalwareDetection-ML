#!/usr/bin/env python
# coding: utf-8

# In[ ]:

# Import necessary packages

import pandas as pd
import numpy as np
import seaborn as sns

from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose  import ColumnTransformer, make_column_selector
from sklearn.pipeline import make_pipeline
from sklearn.metrics import roc_auc_score, plot_roc_curve
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score

import category_encoders as ce

from xgboost import XGBClassifier

from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline as make_pipeline_imb

import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt 

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))


# In[ ]:

# Do necessary preprocessing

df = pd.read_csv("/kaggle/input/malware-detection-tejas/train.csv")
df.set_index('MachineIdentifier', inplace=True)
target = 'HasDetections'
# df.drop(target, axis=1, inplace=True)

test_df = pd.read_csv("/kaggle/input/malware-detection-tejas/test.csv")
test_ids = test_df['MachineIdentifier']
test_df.set_index('MachineIdentifier', inplace=True)

# Dropping columns with more than 60% null values or just 1 unique value

df.drop(['PuaMode', 'Census_ProcessorClass', 'DefaultBrowsersIdentifier', 'Census_IsFlightingInternal', 'Census_InternalBatteryType', 'Census_ThresholdOptIn', 'Census_IsWIMBootEnabled'], axis = 1, inplace = True)
test_df.drop(['PuaMode', 'Census_ProcessorClass', 'DefaultBrowsersIdentifier', 'Census_IsFlightingInternal', 'Census_InternalBatteryType', 'Census_ThresholdOptIn', 'Census_IsWIMBootEnabled'], axis = 1, inplace = True)


df['Census_PrimaryDiskTypeName'].replace(to_replace = ['UNKNOWN', 'Unspecified'], inplace = True)
test_df['Census_PrimaryDiskTypeName'].replace(to_replace = ['UNKNOWN', 'Unspecified'], inplace = True)

df['Census_PrimaryDiskTypeName'].replace(to_replace = ['HDD', 'SSD'], value = [0, 1], inplace = True)
test_df['Census_PrimaryDiskTypeName'].replace(to_replace = ['HDD', 'SSD'], value = [0, 1], inplace = True)

print("Data Shape", df.shape)
df.head()


# In[ ]:


# Scikit-Learn imputers require that the missing values are represented with np.nan hence use of the fillna method.

df.fillna(np.nan, inplace=True)
test_df.fillna(np.nan, inplace=True)


# In[ ]:

# Imputing numerical columns


import matplotlib.pyplot as plt

select_numeric_features = make_column_selector(dtype_include=np.number)
numeric_features = select_numeric_features(df)

numeric_pipeline = make_pipeline(SimpleImputer(strategy='median', add_indicator=True))


# In[ ]:

# Selecting columns in one hot encoding list which have <= 10 unique values

MAX_OH_CARDINALITY = 10

def select_oh_features(df):
    
    hc_features =        df        .select_dtypes(['object', 'category'])        .apply(lambda col: col.nunique())        .loc[lambda x: x <= MAX_OH_CARDINALITY]        .index        .tolist()
        
    return hc_features

oh_features = select_oh_features(df)

print(f'N oh_features: {len(oh_features)} \n')
print(', '.join(oh_features))


# In[ ]:

# Selecting columns in target encoding list which have > 10 unique values

def select_hc_features(df):
    
    hc_features =        df        .select_dtypes(['object', 'category'])        .apply(lambda col: col.nunique())        .loc[lambda x: x > MAX_OH_CARDINALITY]        .index        .tolist()
        
    return hc_features


hc_features = select_hc_features(df)

print(f'N hc_features: {len(hc_features)} \n')
print(', '.join(hc_features))


# In[ ]:

# One hot encoding pipeline

oh_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore'))


# In[ ]:

# Cat Boost Encoding pipeline

hc_pipeline = make_pipeline(ce.CatBoostEncoder())


# In[ ]:

# Column Transformation pipeline


import multiprocessing

column_transformer = ColumnTransformer(transformers=                                       [('numeric_pipeline', numeric_pipeline, select_numeric_features),                                        ('oh_pipeline', oh_pipeline, select_oh_features),                                        ('hc_pipeline', hc_pipeline, select_hc_features)],
                                       n_jobs = multiprocessing.cpu_count(),
                                       remainder='drop')


# In[ ]:

# Removing target column from the list of training columns

train_columns = list(df.columns)
train_columns.remove(target)
print(len(train_columns))


# In[ ]:


# Normalisation

from imblearn.under_sampling import NearMiss
from sklearn.datasets import make_classification
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

# Test Train split for validation

X_train, X_test, y_train, y_test = train_test_split(df[train_columns], df['HasDetections'], test_size=0.05, random_state=42, stratify=df['HasDetections'])

X_train = column_transformer.fit_transform(X_train, y_train)
X_test = column_transformer.transform(X_test)
print(X_train.shape)

X_train = scaler.fit_transform(X_train,y_train)
X_test = scaler.transform(X_test)

# Uncomment the lines below if you want to implement sampling

# sm = SMOTE(random_state=42)
# X_train, y_train = sm.fit_sample(X_train, y_train.ravel())

# nm1 = NearMiss(version=1)
# X_train, y_train = nm1.fit_resample(X_train, y_train)

print(X_train.shape)
print(X_test.shape)


# In[ ]:

# Just a buffer, to match indices. Serves no essential purpose

X_T, X_t, y_T, y_t = train_test_split(df[train_columns], df['HasDetections'], test_size=0.05, random_state=42, stratify=df['HasDetections'])
train_index = X_T.index
train_index
validate_index = X_t.index
print(X_T.head(20),X_t.head(20))


# In[ ]:

# Fill the params in the model

model = XGBClassifier(sampling_method='gradient_based',eta = 0.2, max_depth = 10,verbosity=2, gamma=10, tree_method = 'gpu_hist')


# In[ ]:

# Pass through the pipeline

model_pipeline = make_pipeline(column_transformer, model)

model_pipeline.fit(df[train_columns], df[target])

y_train_pred = model_pipeline.predict_proba(test_df)


# In[ ]:


y_train_pred


# In[ ]:

# Find the predictions for 1's

preds = y_train_pred.transpose()[1]


# In[ ]:


res = pd.DataFrame()
res['MachineIdentifier'] = test_ids
res['HasDetections'] = preds
res


# In[ ]:

# Submission CSV

res.to_csv('test_model_probas_XGB.csv',index=False)

